Usage example: change_mofed_version.sh 4.5-1.0.1
[10/23/2020, 06:04:10 PM] WARNING (nni) Requesting parameter without NNI framework, returning empty dict
2020-10-23 18:04:20 | INFO | fairseq_cli.train | Namespace(activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, allow_tf32=0, arch='mbart_large', attention_dropout=0.1, benchmark=0, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/research/d3/zmwu/model/mbart_company_version/post_process/en-zh_half/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_langtok=True, decoder_layerdrop=0, decoder_layers=12, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.0, empty_cache_freq=4, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_langtok='src', encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model='/research/d3/zmwu/model/mbart_company_version/mbart.cc25/model.pt', fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, inter=1, intra=2, keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=2, keep_last_epochs=-1, label_smoothing=0.1, lang_dict='/research/d3/zmwu/model/mbart_company_version/lang_list', lang_pairs='en_XX-zh_CN,zh_CN-en_XX', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=True, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format='simple', log_interval=10, lr=[1e-05], lr_scheduler='inverse_sqrt', max_epoch=1, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=2048, max_tokens_valid=2048, max_update=20000, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='temperature', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='/research/d3/zmwu/model/mbart_company_version/ckpt/STANDALONE', save_interval=1, save_interval_updates=20000, scoring='bleu', seed=222, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch_nni', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[4], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir='/research/d3/zmwu/model/mbart_company_version/mbart/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=-1, warmup_updates=2500, weight_decay=0.0, zero_sharding='none')
2020-10-23 18:04:20 | INFO | fairseq.data.multilingual.multilingual_data_manager | loaded language list from /research/d3/zmwu/model/mbart_company_version/lang_list as they are ordered in file
2020-10-23 18:04:20 | INFO | fairseq.data.multilingual.multilingual_data_manager | [en_XX] dictionary: 250026 types
2020-10-23 18:04:21 | INFO | fairseq.data.multilingual.multilingual_data_manager | [zh_CN] dictionary: 250026 types
2020-10-23 18:04:21 | INFO | mbart.translation_multi_simple_epoch_nni | loading data for valid epoch=1/None
2020-10-23 18:04:21 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2020-10-23 18:04:21 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:en_XX-zh_CN': 1, 'main:zh_CN-en_XX': 1}
2020-10-23 18:04:21 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:en_XX-zh_CN src_langtok: 250004; tgt_langtok: 250025
2020-10-23 18:04:21 | INFO | fairseq.data.data_utils | loaded 1000000 examples from: /research/d3/zmwu/model/mbart_company_version/post_process/en-zh_half/valid.en_XX-zh_CN.en_XX
2020-10-23 18:04:21 | INFO | fairseq.data.data_utils | loaded 1000000 examples from: /research/d3/zmwu/model/mbart_company_version/post_process/en-zh_half/valid.en_XX-zh_CN.zh_CN
2020-10-23 18:04:21 | INFO | fairseq.data.multilingual.multilingual_data_manager | /research/d3/zmwu/model/mbart_company_version/post_process/en-zh_half/ valid en_XX-zh_CN 1000000 examples
2020-10-23 18:04:21 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:zh_CN-en_XX src_langtok: 250025; tgt_langtok: 250004
2020-10-23 18:04:21 | INFO | fairseq.data.data_utils | loaded 1000000 examples from: /research/d3/zmwu/model/mbart_company_version/post_process/en-zh_half/valid.en_XX-zh_CN.zh_CN
2020-10-23 18:04:22 | INFO | fairseq.data.data_utils | loaded 1000000 examples from: /research/d3/zmwu/model/mbart_company_version/post_process/en-zh_half/valid.en_XX-zh_CN.en_XX
2020-10-23 18:04:22 | INFO | fairseq.data.multilingual.multilingual_data_manager | /research/d3/zmwu/model/mbart_company_version/post_process/en-zh_half/ valid zh_CN-en_XX 1000000 examples
2020-10-23 18:04:46 | INFO | fairseq_cli.train | BARTModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(250026, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
    (layernorm_embedding): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(250026, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
    (layernorm_embedding): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=1024, out_features=250026, bias=False)
  )
  (classification_heads): ModuleDict()
)
2020-10-23 18:04:46 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch_nni (TranslationMultiSimpleEpochNNITask)
2020-10-23 18:04:46 | INFO | fairseq_cli.train | model: mbart_large (BARTModel)
2020-10-23 18:04:46 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-23 18:04:46 | INFO | fairseq_cli.train | num. model params: 610850816 (num. trained: 610850816)
2020-10-23 18:04:53 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-23 18:04:53 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-23 18:04:53 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-23 18:04:53 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2020-10-23 18:04:53 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-23 18:04:53 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-23 18:04:53 | INFO | fairseq_cli.train | max tokens per GPU = 2048 and max sentences per GPU = None
2020-10-23 18:04:53 | INFO | fairseq.checkpoint_utils | loading pretrained model from /research/d3/zmwu/model/mbart_company_version/mbart.cc25/model.pt: optimizer, lr scheduler, meters, dataloader will be reset
2020-10-23 18:05:06 | INFO | fairseq.trainer | loaded checkpoint /research/d3/zmwu/model/mbart_company_version/mbart.cc25/model.pt (epoch 142 @ 0 updates)
2020-10-23 18:05:07 | INFO | fairseq.optim.adam | using FusedAdam
2020-10-23 18:05:07 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-23 18:05:07 | INFO | mbart.translation_multi_simple_epoch_nni | loading data for train epoch=1/None
2020-10-23 18:05:07 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2020-10-23 18:05:07 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:en_XX-zh_CN': 1, 'main:zh_CN-en_XX': 1}
2020-10-23 18:05:07 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:en_XX-zh_CN src_langtok: 250004; tgt_langtok: 250025
2020-10-23 18:05:09 | INFO | fairseq.data.data_utils | loaded 5900000 examples from: /research/d3/zmwu/model/mbart_company_version/post_process/en-zh_half/train.en_XX-zh_CN.en_XX
2020-10-23 18:05:10 | INFO | fairseq.data.data_utils | loaded 5900000 examples from: /research/d3/zmwu/model/mbart_company_version/post_process/en-zh_half/train.en_XX-zh_CN.zh_CN
2020-10-23 18:05:10 | INFO | fairseq.data.multilingual.multilingual_data_manager | /research/d3/zmwu/model/mbart_company_version/post_process/en-zh_half/ train en_XX-zh_CN 5900000 examples
2020-10-23 18:05:10 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:zh_CN-en_XX src_langtok: 250025; tgt_langtok: 250004
2020-10-23 18:05:10 | INFO | fairseq.data.data_utils | loaded 5900000 examples from: /research/d3/zmwu/model/mbart_company_version/post_process/en-zh_half/train.en_XX-zh_CN.zh_CN
2020-10-23 18:05:11 | INFO | fairseq.data.data_utils | loaded 5900000 examples from: /research/d3/zmwu/model/mbart_company_version/post_process/en-zh_half/train.en_XX-zh_CN.en_XX
2020-10-23 18:05:11 | INFO | fairseq.data.multilingual.multilingual_data_manager | /research/d3/zmwu/model/mbart_company_version/post_process/en-zh_half/ train zh_CN-en_XX 5900000 examples
2020-10-23 18:05:11 | INFO | fairseq.data.multilingual.multilingual_data_manager | data sizes multiplied by num_shards used in sampling ratios: [('main:en_XX-zh_CN', 5900000), ('main:zh_CN-en_XX', 5900000)]
2020-10-23 18:05:11 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: temperature
2020-10-23 18:05:11 | INFO | fairseq.data.multilingual.multilingual_data_manager | | Upsample ratios: [('main:en_XX-zh_CN', 0.6299605249474366), ('main:zh_CN-en_XX', 0.6299605249474366)]
2020-10-23 18:05:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000; virtual dataset size 11800000
2020-10-23 18:05:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-23 18:05:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:en_XX-zh_CN': 5900000, 'main:zh_CN-en_XX': 5900000}; raw total size: 11800000
2020-10-23 18:05:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:en_XX-zh_CN': 5900000, 'main:zh_CN-en_XX': 5900000}; resampled total size: 11800000
2020-10-23 18:05:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:en_XX-zh_CN': 0.6299605249474366, 'main:zh_CN-en_XX': 0.6299605249474366}
2020-10-23 18:05:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | sizes() calling time: 0:00:03.617112
2020-10-23 18:05:21 | WARNING | fairseq.tasks.fairseq_task | 10 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[639120, 199661, 195087, 831274, 290465, 233622, 29316, 547539, 697929, 942609]
2020-10-23 18:05:23 | INFO | fairseq.trainer | begin training epoch 1
2020-10-23 18:05:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-10-23 18:05:29 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0
2020-10-23 18:05:30 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0
2020-10-23 18:05:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8.0
2020-10-23 18:05:32 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4.0
2020-10-23 18:05:33 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2.0
2020-10-23 18:05:34 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1.0
2020-10-23 18:05:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.5
2020-10-23 18:05:36 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.25
2020-10-23 18:05:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.125
2020-10-23 18:05:39 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.0625
2020-10-23 18:05:39 | INFO | train_inner | epoch 001:     11 / 4418 loss_scale=0.0625, train_wall=1, wall=45
2020-10-23 18:05:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.03125
2020-10-23 18:05:42 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.015625
2020-10-23 18:05:51 | INFO | train_inner | epoch 001:     23 / 4418 loss=26.279, nll_loss=20.526, ppl=1.51028e+06, wps=6572.6, ups=0.89, wpb=7304, bsz=194.9, num_updates=10, lr=4e-08, gnorm=757.399, loss_scale=0.0156, train_wall=11, wall=57
2020-10-23 18:05:51 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.54 GiB (GPU 0; 23.65 GiB total capacity; 18.87 GiB already allocated; 1.26 GiB free; 21.63 GiB reserved in total by PyTorch)
Exception raised from malloc at ../c10/cuda/CUDACachingAllocator.cpp:272 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x6b (0x7f29d7b4399b in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x268e2 (0x7f29d7d8e8e2 in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #2: <unknown function> + 0x27603 (0x7f29d7d8f603 in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #3: <unknown function> + 0x27ccd (0x7f29d7d8fccd in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #4: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x261 (0x7f29da87b421 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x2a7f629 (0x7f29daa1b629 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x2aa34ff (0x7f29daa3f4ff in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0x2aa21e1 (0x7f29daa3e1e1 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #8: <unknown function> + 0xf3c44e (0x7f2a0a3bb44e in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #9: <unknown function> + 0xf3f811 (0x7f2a0a3be811 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #10: at::empty(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x302 (0x7f2a0a503532 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #11: at::TensorIterator::allocate_outputs() + 0x420 (0x7f2a0a0f4300 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #12: at::TensorIterator::build(at::TensorIteratorConfig&) + 0x1fe (0x7f2a0a0f608e in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #13: at::TensorIterator::TensorIterator(at::TensorIteratorConfig&) + 0xdd (0x7f2a0a0f66ad in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #14: at::TensorIterator::binary_op(at::Tensor&, at::Tensor const&, at::Tensor const&, bool) + 0x15d (0x7f2a0a0f686d in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #15: at::native::add(at::Tensor const&, at::Tensor const&, c10::Scalar) + 0x54 (0x7f2a09dd6a34 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x2a6286f (0x7f29da9fe86f in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #17: <unknown function> + 0x817ed8 (0x7f2a09c96ed8 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #18: at::add(at::Tensor const&, at::Tensor const&, c10::Scalar) + 0x164 (0x7f2a0a4c9514 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0x2525160 (0x7f2a0b9a4160 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #20: <unknown function> + 0x817ed8 (0x7f2a09c96ed8 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #21: at::Tensor::add(at::Tensor const&, c10::Scalar) const + 0x164 (0x7f2a0a67e6d4 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #22: <unknown function> + 0x29e58d8 (0x7f2a0be648d8 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #23: <unknown function> + 0x29e6804 (0x7f2a0be65804 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #24: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0xbbb (0x7f2a0be51f4b in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #25: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x4b4 (0x7f2a0be533f4 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #26: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x99 (0x7f2a0be4aec9 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #27: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x5a (0x7f2a10b8f05a in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so)
frame #28: <unknown function> + 0xbd6df (0x7f2a1cbc96df in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #29: <unknown function> + 0x76db (0x7f2a3f8186db in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #30: clone + 0x3f (0x7f2a3f541a3f in /lib/x86_64-linux-gnu/libc.so.6)

2020-10-23 18:05:51 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 7         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   17746 MB |   21193 MB |    2065 GB |    2048 GB |
|       from large pool |   17743 MB |   21190 MB |    2061 GB |    2044 GB |
|       from small pool |       2 MB |     103 MB |       4 GB |       4 GB |
|---------------------------------------------------------------------------|
| Active memory         |   17746 MB |   21193 MB |    2065 GB |    2048 GB |
|       from large pool |   17743 MB |   21190 MB |    2061 GB |    2044 GB |
|       from small pool |       2 MB |     103 MB |       4 GB |       4 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22150 MB |   22288 MB |  372310 MB |  350160 MB |
|       from large pool |   22146 MB |   22210 MB |  371170 MB |  349024 MB |
|       from small pool |       4 MB |     118 MB |    1140 MB |    1136 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2467 MB |    6101 MB |    1661 GB |    1659 GB |
|       from large pool |    2466 MB |    6096 MB |    1657 GB |    1654 GB |
|       from small pool |       1 MB |      26 MB |       4 GB |       4 GB |
|---------------------------------------------------------------------------|
| Allocations           |    1607    |    1630    |  284945    |  283338    |
|       from large pool |     858    |     872    |  185469    |  184611    |
|       from small pool |     749    |     885    |   99476    |   98727    |
|---------------------------------------------------------------------------|
| Active allocs         |    1607    |    1630    |  284945    |  283338    |
|       from large pool |     858    |     872    |  185469    |  184611    |
|       from small pool |     749    |     885    |   99476    |   98727    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      50    |     214    |    1284    |    1234    |
|       from large pool |      48    |     189    |     714    |     666    |
|       from small pool |       2    |      59    |     570    |     568    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     135    |     170    |  145097    |  144962    |
|       from large pool |      93    |     116    |  105213    |  105120    |
|       from small pool |      42    |      88    |   39884    |   39842    |
|===========================================================================|

2020-10-23 18:05:51 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2020-10-23 18:05:52 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.77 GiB (GPU 0; 23.65 GiB total capacity; 19.55 GiB already allocated; 1.29 GiB free; 21.59 GiB reserved in total by PyTorch)
Exception raised from malloc at ../c10/cuda/CUDACachingAllocator.cpp:272 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x6b (0x7f29d7b4399b in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x268e2 (0x7f29d7d8e8e2 in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #2: <unknown function> + 0x27603 (0x7f29d7d8f603 in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #3: <unknown function> + 0x27ccd (0x7f29d7d8fccd in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #4: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x261 (0x7f29da87b421 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x2a7f629 (0x7f29daa1b629 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x2aa34ff (0x7f29daa3f4ff in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0x2aa21e1 (0x7f29daa3e1e1 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #8: <unknown function> + 0xf3c44e (0x7f2a0a3bb44e in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #9: <unknown function> + 0xf3f811 (0x7f2a0a3be811 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #10: at::empty(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x302 (0x7f2a0a503532 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #11: at::TensorIterator::allocate_outputs() + 0x420 (0x7f2a0a0f4300 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #12: at::TensorIterator::build(at::TensorIteratorConfig&) + 0x1fe (0x7f2a0a0f608e in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #13: at::TensorIterator::TensorIterator(at::TensorIteratorConfig&) + 0xdd (0x7f2a0a0f66ad in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #14: at::TensorIterator::binary_op(at::Tensor&, at::Tensor const&, at::Tensor const&, bool) + 0x15d (0x7f2a0a0f686d in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #15: at::native::add(at::Tensor const&, at::Tensor const&, c10::Scalar) + 0x54 (0x7f2a09dd6a34 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x2a6286f (0x7f29da9fe86f in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #17: <unknown function> + 0x817ed8 (0x7f2a09c96ed8 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #18: at::add(at::Tensor const&, at::Tensor const&, c10::Scalar) + 0x164 (0x7f2a0a4c9514 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0x2525160 (0x7f2a0b9a4160 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #20: <unknown function> + 0x817ed8 (0x7f2a09c96ed8 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #21: at::Tensor::add(at::Tensor const&, c10::Scalar) const + 0x164 (0x7f2a0a67e6d4 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #22: <unknown function> + 0x29e58d8 (0x7f2a0be648d8 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #23: <unknown function> + 0x29e6804 (0x7f2a0be65804 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #24: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0xbbb (0x7f2a0be51f4b in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #25: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x4b4 (0x7f2a0be533f4 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #26: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x99 (0x7f2a0be4aec9 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #27: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x5a (0x7f2a10b8f05a in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so)
frame #28: <unknown function> + 0xbd6df (0x7f2a1cbc96df in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #29: <unknown function> + 0x76db (0x7f2a3f8186db in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #30: clone + 0x3f (0x7f2a3f541a3f in /lib/x86_64-linux-gnu/libc.so.6)

2020-10-23 18:05:52 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 8         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   18198 MB |   21193 MB |    2121 GB |    2103 GB |
|       from large pool |   18195 MB |   21190 MB |    2117 GB |    2099 GB |
|       from small pool |       2 MB |     103 MB |       4 GB |       4 GB |
|---------------------------------------------------------------------------|
| Active memory         |   18198 MB |   21193 MB |    2121 GB |    2103 GB |
|       from large pool |   18195 MB |   21190 MB |    2117 GB |    2099 GB |
|       from small pool |       2 MB |     103 MB |       4 GB |       4 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22112 MB |   22288 MB |  374232 MB |  352120 MB |
|       from large pool |   22108 MB |   22210 MB |  373068 MB |  350960 MB |
|       from small pool |       4 MB |     118 MB |    1164 MB |    1160 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1977 MB |    6101 MB |    1711 GB |    1709 GB |
|       from large pool |    1976 MB |    6096 MB |    1706 GB |    1704 GB |
|       from small pool |       1 MB |      26 MB |       4 GB |       4 GB |
|---------------------------------------------------------------------------|
| Allocations           |    1607    |    1630    |  291973    |  290366    |
|       from large pool |     858    |     872    |  190195    |  189337    |
|       from small pool |     749    |     885    |  101778    |  101029    |
|---------------------------------------------------------------------------|
| Active allocs         |    1607    |    1630    |  291973    |  290366    |
|       from large pool |     858    |     872    |  190195    |  189337    |
|       from small pool |     749    |     885    |  101778    |  101029    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      50    |     214    |    1297    |    1247    |
|       from large pool |      48    |     189    |     715    |     667    |
|       from small pool |       2    |      59    |     582    |     580    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     139    |     170    |  148637    |  148498    |
|       from large pool |     119    |     120    |  107820    |  107701    |
|       from small pool |      20    |      88    |   40817    |   40797    |
|===========================================================================|

2020-10-23 18:05:52 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2020-10-23 18:05:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.0078125
2020-10-23 18:06:03 | INFO | train_inner | epoch 001:     36 / 4418 loss=26.833, nll_loss=21.351, ppl=2.67497e+06, wps=5685.1, ups=0.84, wpb=6795.9, bsz=253.8, num_updates=20, lr=8e-08, gnorm=565.639, loss_scale=0.0078, train_wall=10, wall=69
2020-10-23 18:06:06 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.89 GiB (GPU 0; 23.65 GiB total capacity; 19.81 GiB already allocated; 1.18 GiB free; 21.70 GiB reserved in total by PyTorch)
Exception raised from malloc at ../c10/cuda/CUDACachingAllocator.cpp:272 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x6b (0x7f29d7b4399b in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x268e2 (0x7f29d7d8e8e2 in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #2: <unknown function> + 0x27603 (0x7f29d7d8f603 in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #3: <unknown function> + 0x27ccd (0x7f29d7d8fccd in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #4: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x261 (0x7f29da87b421 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x2a7f629 (0x7f29daa1b629 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x2aa34ff (0x7f29daa3f4ff in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0x2aa21e1 (0x7f29daa3e1e1 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #8: <unknown function> + 0xf3c44e (0x7f2a0a3bb44e in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #9: <unknown function> + 0xf3f811 (0x7f2a0a3be811 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #10: at::empty(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x302 (0x7f2a0a503532 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #11: at::TensorIterator::allocate_outputs() + 0x420 (0x7f2a0a0f4300 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #12: at::TensorIterator::build(at::TensorIteratorConfig&) + 0x1fe (0x7f2a0a0f608e in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #13: at::TensorIterator::TensorIterator(at::TensorIteratorConfig&) + 0xdd (0x7f2a0a0f66ad in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #14: at::TensorIterator::binary_op(at::Tensor&, at::Tensor const&, at::Tensor const&, bool) + 0x15d (0x7f2a0a0f686d in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #15: at::native::add(at::Tensor const&, at::Tensor const&, c10::Scalar) + 0x54 (0x7f2a09dd6a34 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x2a6286f (0x7f29da9fe86f in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #17: <unknown function> + 0x817ed8 (0x7f2a09c96ed8 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #18: at::add(at::Tensor const&, at::Tensor const&, c10::Scalar) + 0x164 (0x7f2a0a4c9514 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0x2525160 (0x7f2a0b9a4160 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #20: <unknown function> + 0x817ed8 (0x7f2a09c96ed8 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #21: at::Tensor::add(at::Tensor const&, c10::Scalar) const + 0x164 (0x7f2a0a67e6d4 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #22: <unknown function> + 0x29e58d8 (0x7f2a0be648d8 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #23: <unknown function> + 0x29e6804 (0x7f2a0be65804 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #24: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0xbbb (0x7f2a0be51f4b in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #25: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x4b4 (0x7f2a0be533f4 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #26: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x99 (0x7f2a0be4aec9 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #27: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x5a (0x7f2a10b8f05a in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so)
frame #28: <unknown function> + 0xbd6df (0x7f2a1cbc96df in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #29: <unknown function> + 0x76db (0x7f2a3f8186db in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #30: clone + 0x3f (0x7f2a3f541a3f in /lib/x86_64-linux-gnu/libc.so.6)

2020-10-23 18:06:06 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 15        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   18356 MB |   21300 MB |    3297 GB |    3279 GB |
|       from large pool |   18353 MB |   21298 MB |    3290 GB |    3272 GB |
|       from small pool |       2 MB |     110 MB |       7 GB |       7 GB |
|---------------------------------------------------------------------------|
| Active memory         |   18356 MB |   21300 MB |    3297 GB |    3279 GB |
|       from large pool |   18353 MB |   21298 MB |    3290 GB |    3272 GB |
|       from small pool |       2 MB |     110 MB |       7 GB |       7 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22224 MB |   22338 MB |  420710 MB |  398486 MB |
|       from large pool |   22210 MB |   22220 MB |  419218 MB |  397008 MB |
|       from small pool |      14 MB |     118 MB |    1492 MB |    1478 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1913 MB |    6167 MB |    2770 GB |    2768 GB |
|       from large pool |    1902 MB |    6165 MB |    2762 GB |    2760 GB |
|       from small pool |      11 MB |      48 MB |       8 GB |       8 GB |
|---------------------------------------------------------------------------|
| Allocations           |    1612    |    1630    |  459136    |  457524    |
|       from large pool |     858    |     872    |  297840    |  296982    |
|       from small pool |     754    |     896    |  161296    |  160542    |
|---------------------------------------------------------------------------|
| Active allocs         |    1612    |    1630    |  459136    |  457524    |
|       from large pool |     858    |     872    |  297840    |  296982    |
|       from small pool |     754    |     896    |  161296    |  160542    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      55    |     214    |    1485    |    1430    |
|       from large pool |      48    |     189    |     739    |     691    |
|       from small pool |       7    |      59    |     746    |     739    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     110    |     171    |  234495    |  234385    |
|       from large pool |      96    |     123    |  167634    |  167538    |
|       from small pool |      14    |     108    |   66861    |   66847    |
|===========================================================================|

2020-10-23 18:06:06 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2020-10-23 18:06:14 | INFO | train_inner | epoch 001:     47 / 4418 loss=26.313, nll_loss=20.29, ppl=1.2817e+06, wps=6586.8, ups=0.91, wpb=7249.6, bsz=191.9, num_updates=30, lr=1.2e-07, gnorm=522.186, loss_scale=0.0078, train_wall=10, wall=80
2020-10-23 18:06:24 | INFO | train_inner | epoch 001:     57 / 4418 loss=26.617, nll_loss=20.776, ppl=1.79544e+06, wps=6985.2, ups=0.98, wpb=7151.2, bsz=187.2, num_updates=40, lr=1.6e-07, gnorm=571.932, loss_scale=0.0078, train_wall=10, wall=91
2020-10-23 18:06:35 | INFO | train_inner | epoch 001:     67 / 4418 loss=25.48, nll_loss=19.886, ppl=968854, wps=6878.2, ups=0.94, wpb=7334.9, bsz=203.1, num_updates=50, lr=2e-07, gnorm=605.205, loss_scale=0.0078, train_wall=10, wall=101
2020-10-23 18:06:45 | INFO | train_inner | epoch 001:     77 / 4418 loss=27.12, nll_loss=20.636, ppl=1.62942e+06, wps=7414.6, ups=0.99, wpb=7463.4, bsz=206.9, num_updates=60, lr=2.4e-07, gnorm=643.814, loss_scale=0.0078, train_wall=10, wall=111
2020-10-23 18:06:47 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.00390625
2020-10-23 18:06:48 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.66 GiB (GPU 0; 23.65 GiB total capacity; 20.50 GiB already allocated; 1.53 GiB free; 21.36 GiB reserved in total by PyTorch)
Exception raised from malloc at ../c10/cuda/CUDACachingAllocator.cpp:272 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x6b (0x7f29d7b4399b in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x268e2 (0x7f29d7d8e8e2 in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #2: <unknown function> + 0x27603 (0x7f29d7d8f603 in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #3: <unknown function> + 0x27ccd (0x7f29d7d8fccd in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #4: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x261 (0x7f29da87b421 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x2a7f629 (0x7f29daa1b629 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x2aa34ff (0x7f29daa3f4ff in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0x2aa21e1 (0x7f29daa3e1e1 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #8: <unknown function> + 0xf3c44e (0x7f2a0a3bb44e in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #9: <unknown function> + 0xf3f811 (0x7f2a0a3be811 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #10: at::empty(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x302 (0x7f2a0a503532 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #11: at::TensorIterator::allocate_outputs() + 0x420 (0x7f2a0a0f4300 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #12: at::TensorIterator::build(at::TensorIteratorConfig&) + 0x1fe (0x7f2a0a0f608e in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #13: at::TensorIterator::TensorIterator(at::TensorIteratorConfig&) + 0xdd (0x7f2a0a0f66ad in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #14: at::TensorIterator::binary_op(at::Tensor&, at::Tensor const&, at::Tensor const&, bool) + 0x15d (0x7f2a0a0f686d in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #15: at::native::add(at::Tensor const&, at::Tensor const&, c10::Scalar) + 0x54 (0x7f2a09dd6a34 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x2a6286f (0x7f29da9fe86f in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #17: <unknown function> + 0x817ed8 (0x7f2a09c96ed8 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #18: at::add(at::Tensor const&, at::Tensor const&, c10::Scalar) + 0x164 (0x7f2a0a4c9514 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0x2525160 (0x7f2a0b9a4160 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #20: <unknown function> + 0x817ed8 (0x7f2a09c96ed8 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #21: at::Tensor::add(at::Tensor const&, c10::Scalar) const + 0x164 (0x7f2a0a67e6d4 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #22: <unknown function> + 0x29e58d8 (0x7f2a0be648d8 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #23: <unknown function> + 0x29e6804 (0x7f2a0be65804 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #24: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0xbbb (0x7f2a0be51f4b in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #25: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x4b4 (0x7f2a0be533f4 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #26: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x99 (0x7f2a0be4aec9 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #27: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x5a (0x7f2a10b8f05a in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so)
frame #28: <unknown function> + 0xbd6df (0x7f2a1cbc96df in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #29: <unknown function> + 0x76db (0x7f2a3f8186db in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #30: clone + 0x3f (0x7f2a3f541a3f in /lib/x86_64-linux-gnu/libc.so.6)

2020-10-23 18:06:48 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 41        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   19291 MB |   21786 MB |    6877 GB |    6859 GB |
|       from large pool |   19288 MB |   21783 MB |    6862 GB |    6843 GB |
|       from small pool |       3 MB |     166 MB |      15 GB |      15 GB |
|---------------------------------------------------------------------------|
| Active memory         |   19291 MB |   21786 MB |    6877 GB |    6859 GB |
|       from large pool |   19288 MB |   21783 MB |    6862 GB |    6843 GB |
|       from small pool |       3 MB |     166 MB |      15 GB |      15 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21868 MB |   23224 MB |  614318 MB |  592450 MB |
|       from large pool |   21860 MB |   23118 MB |  611786 MB |  589926 MB |
|       from small pool |       8 MB |     170 MB |    2532 MB |    2524 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  637621 KB |    6609 MB |    5829 GB |    5829 GB |
|       from large pool |  632582 KB |    6608 MB |    5813 GB |    5812 GB |
|       from small pool |    5039 KB |      48 MB |      16 GB |      16 GB |
|---------------------------------------------------------------------------|
| Allocations           |    1608    |    1630    |     958 K  |     956 K  |
|       from large pool |     858    |     873    |     620 K  |     620 K  |
|       from small pool |     750    |     975    |     337 K  |     336 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1608    |    1630    |     958 K  |     956 K  |
|       from large pool |     858    |     873    |     620 K  |     620 K  |
|       from small pool |     750    |     975    |     337 K  |     336 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      52    |     214    |    2110    |    2058    |
|       from large pool |      48    |     189    |     844    |     796    |
|       from small pool |       4    |      85    |    1266    |    1262    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     122    |     182    |  494313    |  494191    |
|       from large pool |      87    |     123    |  348846    |  348759    |
|       from small pool |      35    |     108    |  145467    |  145432    |
|===========================================================================|

2020-10-23 18:06:48 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2020-10-23 18:06:49 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.52 GiB (GPU 0; 23.65 GiB total capacity; 20.48 GiB already allocated; 1.26 GiB free; 21.63 GiB reserved in total by PyTorch)
Exception raised from malloc at ../c10/cuda/CUDACachingAllocator.cpp:272 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x6b (0x7f29d7b4399b in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x268e2 (0x7f29d7d8e8e2 in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #2: <unknown function> + 0x27603 (0x7f29d7d8f603 in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #3: <unknown function> + 0x27ccd (0x7f29d7d8fccd in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #4: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x261 (0x7f29da87b421 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x2a7f629 (0x7f29daa1b629 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x2aa34ff (0x7f29daa3f4ff in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0x2aa21e1 (0x7f29daa3e1e1 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #8: <unknown function> + 0xf3c44e (0x7f2a0a3bb44e in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #9: <unknown function> + 0xf3f811 (0x7f2a0a3be811 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #10: at::empty(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x302 (0x7f2a0a503532 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #11: at::TensorIterator::allocate_outputs() + 0x420 (0x7f2a0a0f4300 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #12: at::TensorIterator::build(at::TensorIteratorConfig&) + 0x1fe (0x7f2a0a0f608e in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #13: at::TensorIterator::TensorIterator(at::TensorIteratorConfig&) + 0xdd (0x7f2a0a0f66ad in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #14: at::TensorIterator::binary_op(at::Tensor&, at::Tensor const&, at::Tensor const&, bool) + 0x15d (0x7f2a0a0f686d in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #15: at::native::add(at::Tensor const&, at::Tensor const&, c10::Scalar) + 0x54 (0x7f2a09dd6a34 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x2a6286f (0x7f29da9fe86f in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #17: <unknown function> + 0x817ed8 (0x7f2a09c96ed8 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #18: at::add(at::Tensor const&, at::Tensor const&, c10::Scalar) + 0x164 (0x7f2a0a4c9514 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0x2525160 (0x7f2a0b9a4160 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #20: <unknown function> + 0x817ed8 (0x7f2a09c96ed8 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #21: at::Tensor::add(at::Tensor const&, c10::Scalar) const + 0x164 (0x7f2a0a67e6d4 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #22: <unknown function> + 0x29e58d8 (0x7f2a0be648d8 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #23: <unknown function> + 0x29e6804 (0x7f2a0be65804 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #24: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0xbbb (0x7f2a0be51f4b in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #25: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x4b4 (0x7f2a0be533f4 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #26: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x99 (0x7f2a0be4aec9 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #27: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x5a (0x7f2a10b8f05a in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so)
frame #28: <unknown function> + 0xbd6df (0x7f2a1cbc96df in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #29: <unknown function> + 0x76db (0x7f2a3f8186db in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #30: clone + 0x3f (0x7f2a3f541a3f in /lib/x86_64-linux-gnu/libc.so.6)

2020-10-23 18:06:49 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 44        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   19411 MB |   21786 MB |    6954 GB |    6935 GB |
|       from large pool |   19408 MB |   21783 MB |    6939 GB |    6920 GB |
|       from small pool |       3 MB |     166 MB |      15 GB |      15 GB |
|---------------------------------------------------------------------------|
| Active memory         |   19411 MB |   21786 MB |    6954 GB |    6935 GB |
|       from large pool |   19408 MB |   21783 MB |    6939 GB |    6920 GB |
|       from small pool |       3 MB |     166 MB |      15 GB |      15 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22150 MB |   23224 MB |  621746 MB |  599596 MB |
|       from large pool |   22140 MB |   23118 MB |  619138 MB |  596998 MB |
|       from small pool |      10 MB |     170 MB |    2608 MB |    2598 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |     784 MB |    6609 MB |    5897 GB |    5896 GB |
|       from large pool |     777 MB |    6608 MB |    5880 GB |    5879 GB |
|       from small pool |       6 MB |      48 MB |      17 GB |      17 GB |
|---------------------------------------------------------------------------|
| Allocations           |    1612    |    1630    |     968 K  |     966 K  |
|       from large pool |     858    |     873    |     627 K  |     626 K  |
|       from small pool |     754    |     975    |     341 K  |     340 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1612    |    1630    |     968 K  |     966 K  |
|       from large pool |     858    |     873    |     627 K  |     626 K  |
|       from small pool |     754    |     975    |     341 K  |     340 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      53    |     214    |    2152    |    2099    |
|       from large pool |      48    |     189    |     848    |     800    |
|       from small pool |       5    |      85    |    1304    |    1299    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     137    |     182    |  499137    |  499000    |
|       from large pool |      92    |     123    |  352331    |  352239    |
|       from small pool |      45    |     108    |  146806    |  146761    |
|===========================================================================|

2020-10-23 18:06:49 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2020-10-23 18:06:57 | INFO | train_inner | epoch 001:     90 / 4418 loss=23.852, nll_loss=18.418, ppl=350295, wps=5743.4, ups=0.81, wpb=7071.8, bsz=261.2, num_updates=70, lr=2.8e-07, gnorm=552.933, loss_scale=0.0039, train_wall=11, wall=124
2020-10-23 18:07:07 | INFO | train_inner | epoch 001:    100 / 4418 loss=23.135, nll_loss=17.847, ppl=235697, wps=7063.7, ups=0.97, wpb=7298.8, bsz=234.6, num_updates=80, lr=3.2e-07, gnorm=612.78, loss_scale=0.0039, train_wall=10, wall=134
2020-10-23 18:07:18 | INFO | train_inner | epoch 001:    110 / 4418 loss=21.838, nll_loss=16.7, ppl=106448, wps=7041.7, ups=0.96, wpb=7315.7, bsz=261.6, num_updates=90, lr=3.6e-07, gnorm=460.694, loss_scale=0.0039, train_wall=10, wall=144
2020-10-23 18:07:28 | INFO | train_inner | epoch 001:    120 / 4418 loss=19.329, nll_loss=14.86, ppl=29733.3, wps=6939.6, ups=0.94, wpb=7407.7, bsz=224.2, num_updates=100, lr=4e-07, gnorm=437.775, loss_scale=0.0039, train_wall=10, wall=155
2020-10-23 18:07:32 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.90 GiB (GPU 0; 23.65 GiB total capacity; 20.10 GiB already allocated; 1.57 GiB free; 21.32 GiB reserved in total by PyTorch)
Exception raised from malloc at ../c10/cuda/CUDACachingAllocator.cpp:272 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x6b (0x7f29d7b4399b in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x268e2 (0x7f29d7d8e8e2 in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #2: <unknown function> + 0x27603 (0x7f29d7d8f603 in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #3: <unknown function> + 0x27ccd (0x7f29d7d8fccd in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #4: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x261 (0x7f29da87b421 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x2a7f629 (0x7f29daa1b629 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x2aa34ff (0x7f29daa3f4ff in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0x2aa21e1 (0x7f29daa3e1e1 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #8: <unknown function> + 0xf3c44e (0x7f2a0a3bb44e in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #9: <unknown function> + 0xf3f811 (0x7f2a0a3be811 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #10: at::empty(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x302 (0x7f2a0a503532 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #11: at::TensorIterator::allocate_outputs() + 0x420 (0x7f2a0a0f4300 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #12: at::TensorIterator::build(at::TensorIteratorConfig&) + 0x1fe (0x7f2a0a0f608e in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #13: at::TensorIterator::TensorIterator(at::TensorIteratorConfig&) + 0xdd (0x7f2a0a0f66ad in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #14: at::TensorIterator::binary_op(at::Tensor&, at::Tensor const&, at::Tensor const&, bool) + 0x15d (0x7f2a0a0f686d in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #15: at::native::add(at::Tensor const&, at::Tensor const&, c10::Scalar) + 0x54 (0x7f2a09dd6a34 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x2a6286f (0x7f29da9fe86f in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #17: <unknown function> + 0x817ed8 (0x7f2a09c96ed8 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #18: at::add(at::Tensor const&, at::Tensor const&, c10::Scalar) + 0x164 (0x7f2a0a4c9514 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0x2525160 (0x7f2a0b9a4160 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #20: <unknown function> + 0x817ed8 (0x7f2a09c96ed8 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #21: at::Tensor::add(at::Tensor const&, c10::Scalar) const + 0x164 (0x7f2a0a67e6d4 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #22: <unknown function> + 0x29e58d8 (0x7f2a0be648d8 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #23: <unknown function> + 0x29e6804 (0x7f2a0be65804 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #24: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0xbbb (0x7f2a0be51f4b in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #25: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x4b4 (0x7f2a0be533f4 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #26: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x99 (0x7f2a0be4aec9 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #27: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x5a (0x7f2a10b8f05a in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so)
frame #28: <unknown function> + 0xbd6df (0x7f2a1cbc96df in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #29: <unknown function> + 0x76db (0x7f2a3f8186db in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #30: clone + 0x3f (0x7f2a3f541a3f in /lib/x86_64-linux-gnu/libc.so.6)

2020-10-23 18:07:32 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 75        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   18633 MB |   21786 MB |   10525 GB |   10507 GB |
|       from large pool |   18629 MB |   21783 MB |   10503 GB |   10485 GB |
|       from small pool |       3 MB |     166 MB |      22 GB |      22 GB |
|---------------------------------------------------------------------------|
| Active memory         |   18633 MB |   21786 MB |   10525 GB |   10507 GB |
|       from large pool |   18629 MB |   21783 MB |   10503 GB |   10485 GB |
|       from small pool |       3 MB |     166 MB |      22 GB |      22 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21830 MB |   23282 MB |     828 GB |     807 GB |
|       from large pool |   21824 MB |   23220 MB |     825 GB |     803 GB |
|       from small pool |       6 MB |     170 MB |       3 GB |       3 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1246 MB |    6609 MB |    8995 GB |    8993 GB |
|       from large pool |    1244 MB |    6608 MB |    8970 GB |    8969 GB |
|       from small pool |       2 MB |      48 MB |      24 GB |      24 GB |
|---------------------------------------------------------------------------|
| Allocations           |    1608    |    1630    |    1468 K  |    1466 K  |
|       from large pool |     858    |     873    |     951 K  |     950 K  |
|       from small pool |     750    |     975    |     516 K  |     515 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1608    |    1630    |    1468 K  |    1466 K  |
|       from large pool |     858    |     873    |     951 K  |     950 K  |
|       from small pool |     750    |     975    |     516 K  |     515 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      51    |     214    |    2705    |    2654    |
|       from large pool |      48    |     189    |     971    |     923    |
|       from small pool |       3    |      85    |    1734    |    1731    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     108    |     182    |     759 K  |     759 K  |
|       from large pool |      80    |     123    |     535 K  |     535 K  |
|       from small pool |      28    |     108    |     224 K  |     224 K  |
|===========================================================================|

2020-10-23 18:07:32 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2020-10-23 18:07:39 | INFO | train_inner | epoch 001:    131 / 4418 loss=17.734, nll_loss=13.988, ppl=16248, wps=6835.1, ups=0.92, wpb=7410.5, bsz=234.2, num_updates=110, lr=4.4e-07, gnorm=324.147, loss_scale=0.0039, train_wall=10, wall=166
2020-10-23 18:07:50 | INFO | train_inner | epoch 001:    141 / 4418 loss=16.547, nll_loss=13.161, ppl=9157.02, wps=7057.2, ups=0.96, wpb=7361.7, bsz=211.4, num_updates=120, lr=4.8e-07, gnorm=213.69, loss_scale=0.0039, train_wall=10, wall=176
2020-10-23 18:08:00 | INFO | train_inner | epoch 001:    151 / 4418 loss=16.419, nll_loss=13.273, ppl=9897.69, wps=6893.2, ups=0.96, wpb=7205.9, bsz=245, num_updates=130, lr=5.2e-07, gnorm=239.296, loss_scale=0.0039, train_wall=10, wall=187
2020-10-23 18:08:11 | INFO | train_inner | epoch 001:    161 / 4418 loss=16.026, nll_loss=13.05, ppl=8482.94, wps=6442.6, ups=0.92, wpb=7038.5, bsz=251.7, num_updates=140, lr=5.6e-07, gnorm=186.681, loss_scale=0.0039, train_wall=11, wall=198
2020-10-23 18:08:19 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.89 GiB (GPU 0; 23.65 GiB total capacity; 20.35 GiB already allocated; 1.66 GiB free; 21.23 GiB reserved in total by PyTorch)
Exception raised from malloc at ../c10/cuda/CUDACachingAllocator.cpp:272 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x6b (0x7f29d7b4399b in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x268e2 (0x7f29d7d8e8e2 in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #2: <unknown function> + 0x27603 (0x7f29d7d8f603 in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #3: <unknown function> + 0x27ccd (0x7f29d7d8fccd in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #4: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x261 (0x7f29da87b421 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x2a7f629 (0x7f29daa1b629 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x2aa34ff (0x7f29daa3f4ff in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0x2aa21e1 (0x7f29daa3e1e1 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #8: <unknown function> + 0xf3c44e (0x7f2a0a3bb44e in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #9: <unknown function> + 0xf3f811 (0x7f2a0a3be811 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #10: at::empty(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x302 (0x7f2a0a503532 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #11: <unknown function> + 0x2593dad (0x7f2a0ba12dad in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #12: <unknown function> + 0x25c9bf6 (0x7f2a0ba48bf6 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #13: <unknown function> + 0xf3f811 (0x7f2a0a3be811 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #14: at::empty(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x302 (0x7f2a0a503532 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #15: at::native::zeros(c10::ArrayRef<long>, c10::TensorOptions const&) + 0x35 (0x7f2a0a0d9525 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x11257f3 (0x7f2a0a5a47f3 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x25863ad (0x7f2a0ba053ad in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0xf3e9c3 (0x7f2a0a3bd9c3 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0x10f0e17 (0x7f2a0a56fe17 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #20: at::zeros(c10::ArrayRef<long>, c10::TensorOptions const&) + 0xc1 (0x7f2a0a4fd4a1 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #21: torch::autograd::generated::GatherBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x229 (0x7f2a0b82b559 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #22: <unknown function> + 0x29d82c5 (0x7f2a0be572c5 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #23: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x14a8 (0x7f2a0be52838 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #24: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x4b4 (0x7f2a0be533f4 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #25: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x99 (0x7f2a0be4aec9 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #26: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x5a (0x7f2a10b8f05a in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so)
frame #27: <unknown function> + 0xbd6df (0x7f2a1cbc96df in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #28: <unknown function> + 0x76db (0x7f2a3f8186db in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #29: clone + 0x3f (0x7f2a3f541a3f in /lib/x86_64-linux-gnu/libc.so.6)

2020-10-23 18:08:19 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 7            |        cudaMalloc retries: 109       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   20836 MB |   21786 MB |   14419 GB |   14398 GB |
|       from large pool |   20833 MB |   21783 MB |   14388 GB |   14368 GB |
|       from small pool |       2 MB |     166 MB |      30 GB |      30 GB |
|---------------------------------------------------------------------------|
| Active memory         |   20836 MB |   21786 MB |   14419 GB |   14398 GB |
|       from large pool |   20833 MB |   21783 MB |   14388 GB |   14368 GB |
|       from small pool |       2 MB |     166 MB |      30 GB |      30 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21740 MB |   23436 MB |    1067 GB |    1046 GB |
|       from large pool |   21730 MB |   23416 MB |    1062 GB |    1041 GB |
|       from small pool |      10 MB |     170 MB |       4 GB |       4 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |     903 MB |    6609 MB |   12385 GB |   12384 GB |
|       from large pool |     896 MB |    6608 MB |   12351 GB |   12350 GB |
|       from small pool |       7 MB |      48 MB |      34 GB |      34 GB |
|---------------------------------------------------------------------------|
| Allocations           |    1608    |    1630    |    2016 K  |    2014 K  |
|       from large pool |     859    |     874    |    1305 K  |    1304 K  |
|       from small pool |     749    |     975    |     710 K  |     709 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1608    |    1630    |    2016 K  |    2014 K  |
|       from large pool |     859    |     874    |    1305 K  |    1304 K  |
|       from small pool |     749    |     975    |     710 K  |     709 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      53    |     214    |    3464    |    3411    |
|       from large pool |      48    |     189    |    1104    |    1056    |
|       from small pool |       5    |      85    |    2360    |    2355    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     147    |     182    |    1042 K  |    1042 K  |
|       from large pool |     101    |     123    |     733 K  |     733 K  |
|       from small pool |      46    |     108    |     308 K  |     308 K  |
|===========================================================================|

2020-10-23 18:08:19 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2020-10-23 18:08:21 | INFO | train_inner | epoch 001:    172 / 4418 loss=14.293, nll_loss=11.569, ppl=3037.46, wps=7051.4, ups=0.99, wpb=7130.3, bsz=171.8, num_updates=150, lr=6e-07, gnorm=156.226, loss_scale=0.0039, train_wall=9, wall=208
2020-10-23 18:08:23 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.77 GiB (GPU 0; 23.65 GiB total capacity; 19.02 GiB already allocated; 1.73 GiB free; 21.16 GiB reserved in total by PyTorch)
Exception raised from malloc at ../c10/cuda/CUDACachingAllocator.cpp:272 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x6b (0x7f29d7b4399b in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x268e2 (0x7f29d7d8e8e2 in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #2: <unknown function> + 0x27603 (0x7f29d7d8f603 in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #3: <unknown function> + 0x27ccd (0x7f29d7d8fccd in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #4: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x261 (0x7f29da87b421 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x2a7f629 (0x7f29daa1b629 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x2aa34ff (0x7f29daa3f4ff in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0x2aa21e1 (0x7f29daa3e1e1 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #8: <unknown function> + 0xf3c44e (0x7f2a0a3bb44e in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #9: <unknown function> + 0xf3f811 (0x7f2a0a3be811 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #10: at::empty(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x302 (0x7f2a0a503532 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #11: at::TensorIterator::allocate_outputs() + 0x420 (0x7f2a0a0f4300 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #12: at::TensorIterator::build(at::TensorIteratorConfig&) + 0x1fe (0x7f2a0a0f608e in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #13: at::TensorIterator::TensorIterator(at::TensorIteratorConfig&) + 0xdd (0x7f2a0a0f66ad in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #14: at::TensorIterator::binary_op(at::Tensor&, at::Tensor const&, at::Tensor const&, bool) + 0x15d (0x7f2a0a0f686d in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #15: at::native::add(at::Tensor const&, at::Tensor const&, c10::Scalar) + 0x54 (0x7f2a09dd6a34 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x2a6286f (0x7f29da9fe86f in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #17: <unknown function> + 0x817ed8 (0x7f2a09c96ed8 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #18: at::add(at::Tensor const&, at::Tensor const&, c10::Scalar) + 0x164 (0x7f2a0a4c9514 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0x2525160 (0x7f2a0b9a4160 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #20: <unknown function> + 0x817ed8 (0x7f2a09c96ed8 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #21: at::Tensor::add(at::Tensor const&, c10::Scalar) const + 0x164 (0x7f2a0a67e6d4 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #22: <unknown function> + 0x29e58d8 (0x7f2a0be648d8 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #23: <unknown function> + 0x29e6804 (0x7f2a0be65804 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #24: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0xbbb (0x7f2a0be51f4b in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #25: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x4b4 (0x7f2a0be533f4 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #26: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x99 (0x7f2a0be4aec9 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #27: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x5a (0x7f2a10b8f05a in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so)
frame #28: <unknown function> + 0xbd6df (0x7f2a1cbc96df in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #29: <unknown function> + 0x76db (0x7f2a3f8186db in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #30: clone + 0x3f (0x7f2a3f541a3f in /lib/x86_64-linux-gnu/libc.so.6)

2020-10-23 18:08:23 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 8            |        cudaMalloc retries: 114       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   17667 MB |   21786 MB |   14755 GB |   14738 GB |
|       from large pool |   17665 MB |   21783 MB |   14723 GB |   14706 GB |
|       from small pool |       2 MB |     166 MB |      31 GB |      31 GB |
|---------------------------------------------------------------------------|
| Active memory         |   17667 MB |   21786 MB |   14755 GB |   14738 GB |
|       from large pool |   17665 MB |   21783 MB |   14723 GB |   14706 GB |
|       from small pool |       2 MB |     166 MB |      31 GB |      31 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21670 MB |   23436 MB |    1098 GB |    1077 GB |
|       from large pool |   21662 MB |   23416 MB |    1093 GB |    1072 GB |
|       from small pool |       8 MB |     170 MB |       4 GB |       4 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2048 MB |    6609 MB |   12678 GB |   12676 GB |
|       from large pool |    2042 MB |    6608 MB |   12643 GB |   12641 GB |
|       from small pool |       5 MB |      48 MB |      35 GB |      35 GB |
|---------------------------------------------------------------------------|
| Allocations           |    1612    |    1630    |    2062 K  |    2060 K  |
|       from large pool |     859    |     874    |    1335 K  |    1334 K  |
|       from small pool |     753    |     975    |     727 K  |     726 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1612    |    1630    |    2062 K  |    2060 K  |
|       from large pool |     859    |     874    |    1335 K  |    1334 K  |
|       from small pool |     753    |     975    |     727 K  |     726 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      52    |     214    |    3560    |    3508    |
|       from large pool |      48    |     189    |    1122    |    1074    |
|       from small pool |       4    |      85    |    2438    |    2434    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     160    |     182    |    1065 K  |    1065 K  |
|       from large pool |     127    |     128    |     750 K  |     750 K  |
|       from small pool |      33    |     108    |     315 K  |     315 K  |
|===========================================================================|

2020-10-23 18:08:23 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2020-10-23 18:08:32 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.39 GiB (GPU 0; 23.65 GiB total capacity; 19.33 GiB already allocated; 783.44 MiB free; 22.12 GiB reserved in total by PyTorch)
Exception raised from malloc at ../c10/cuda/CUDACachingAllocator.cpp:272 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x6b (0x7f29d7b4399b in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x268e2 (0x7f29d7d8e8e2 in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #2: <unknown function> + 0x27603 (0x7f29d7d8f603 in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #3: <unknown function> + 0x27ccd (0x7f29d7d8fccd in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #4: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x261 (0x7f29da87b421 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x2a7f629 (0x7f29daa1b629 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x2aa34ff (0x7f29daa3f4ff in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0x2aa21e1 (0x7f29daa3e1e1 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #8: <unknown function> + 0xf3c44e (0x7f2a0a3bb44e in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #9: <unknown function> + 0xf3f811 (0x7f2a0a3be811 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #10: at::empty(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x302 (0x7f2a0a503532 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #11: at::TensorIterator::allocate_outputs() + 0x420 (0x7f2a0a0f4300 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #12: at::TensorIterator::build(at::TensorIteratorConfig&) + 0x1fe (0x7f2a0a0f608e in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #13: at::TensorIterator::TensorIterator(at::TensorIteratorConfig&) + 0xdd (0x7f2a0a0f66ad in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #14: at::TensorIterator::binary_op(at::Tensor&, at::Tensor const&, at::Tensor const&, bool) + 0x15d (0x7f2a0a0f686d in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #15: at::native::add(at::Tensor const&, at::Tensor const&, c10::Scalar) + 0x54 (0x7f2a09dd6a34 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x2a6286f (0x7f29da9fe86f in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #17: <unknown function> + 0x817ed8 (0x7f2a09c96ed8 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #18: at::add(at::Tensor const&, at::Tensor const&, c10::Scalar) + 0x164 (0x7f2a0a4c9514 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0x2525160 (0x7f2a0b9a4160 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #20: <unknown function> + 0x817ed8 (0x7f2a09c96ed8 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #21: at::Tensor::add(at::Tensor const&, c10::Scalar) const + 0x164 (0x7f2a0a67e6d4 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #22: <unknown function> + 0x29e58d8 (0x7f2a0be648d8 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #23: <unknown function> + 0x29e6804 (0x7f2a0be65804 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #24: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0xbbb (0x7f2a0be51f4b in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #25: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x4b4 (0x7f2a0be533f4 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #26: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x99 (0x7f2a0be4aec9 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #27: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x5a (0x7f2a10b8f05a in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so)
frame #28: <unknown function> + 0xbd6df (0x7f2a1cbc96df in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #29: <unknown function> + 0x76db (0x7f2a3f8186db in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #30: clone + 0x3f (0x7f2a3f541a3f in /lib/x86_64-linux-gnu/libc.so.6)

2020-10-23 18:08:32 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 9            |        cudaMalloc retries: 120       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   18375 MB |   21786 MB |   15475 GB |   15457 GB |
|       from large pool |   18373 MB |   21783 MB |   15442 GB |   15424 GB |
|       from small pool |       2 MB |     166 MB |      33 GB |      33 GB |
|---------------------------------------------------------------------------|
| Active memory         |   18375 MB |   21786 MB |   15475 GB |   15457 GB |
|       from large pool |   18373 MB |   21783 MB |   15442 GB |   15424 GB |
|       from small pool |       2 MB |     166 MB |      33 GB |      33 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22654 MB |   23436 MB |    1139 GB |    1117 GB |
|       from large pool |   22638 MB |   23416 MB |    1134 GB |    1112 GB |
|       from small pool |      16 MB |     170 MB |       4 GB |       4 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2328 MB |    7276 MB |   13325 GB |   13323 GB |
|       from large pool |    2314 MB |    7268 MB |   13288 GB |   13285 GB |
|       from small pool |      13 MB |      48 MB |      37 GB |      37 GB |
|---------------------------------------------------------------------------|
| Allocations           |    1608    |    1630    |    2163 K  |    2161 K  |
|       from large pool |     859    |     874    |    1400 K  |    1399 K  |
|       from small pool |     749    |     975    |     762 K  |     762 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1608    |    1630    |    2163 K  |    2161 K  |
|       from large pool |     859    |     874    |    1400 K  |    1399 K  |
|       from small pool |     749    |     975    |     762 K  |     762 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      56    |     214    |    3695    |    3639    |
|       from large pool |      48    |     189    |    1144    |    1096    |
|       from small pool |       8    |      85    |    2551    |    2543    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     138    |     182    |    1118 K  |    1117 K  |
|       from large pool |     101    |     132    |     786 K  |     786 K  |
|       from small pool |      37    |     108    |     331 K  |     331 K  |
|===========================================================================|

2020-10-23 18:08:32 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2020-10-23 18:08:33 | INFO | train_inner | epoch 001:    184 / 4418 loss=14.811, nll_loss=12.061, ppl=4274.24, wps=6139.6, ups=0.85, wpb=7229.4, bsz=234.4, num_updates=160, lr=6.4e-07, gnorm=262.193, loss_scale=0.0039, train_wall=10, wall=220
2020-10-23 18:08:44 | INFO | train_inner | epoch 001:    194 / 4418 loss=14.288, nll_loss=11.569, ppl=3038.93, wps=7073.9, ups=0.95, wpb=7467.3, bsz=224.1, num_updates=170, lr=6.8e-07, gnorm=273.77, loss_scale=0.0039, train_wall=10, wall=230
2020-10-23 18:08:54 | INFO | train_inner | epoch 001:    204 / 4418 loss=13.424, nll_loss=10.941, ppl=1966.59, wps=6959.7, ups=0.97, wpb=7201.1, bsz=188.4, num_updates=180, lr=7.2e-07, gnorm=103.475, loss_scale=0.0039, train_wall=10, wall=241
2020-10-23 18:09:04 | INFO | train_inner | epoch 001:    214 / 4418 loss=13.185, nll_loss=10.75, ppl=1722.06, wps=7108.2, ups=0.98, wpb=7288.8, bsz=191.9, num_updates=190, lr=7.6e-07, gnorm=102.537, loss_scale=0.0039, train_wall=10, wall=251
2020-10-23 18:09:05 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.41 GiB (GPU 0; 23.65 GiB total capacity; 17.22 GiB already allocated; 1.41 GiB free; 21.48 GiB reserved in total by PyTorch)
Exception raised from malloc at ../c10/cuda/CUDACachingAllocator.cpp:272 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x6b (0x7f29d7b4399b in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x268e2 (0x7f29d7d8e8e2 in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #2: <unknown function> + 0x27603 (0x7f29d7d8f603 in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #3: <unknown function> + 0x27ccd (0x7f29d7d8fccd in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #4: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x261 (0x7f29da87b421 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x2a7f629 (0x7f29daa1b629 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x2aa34ff (0x7f29daa3f4ff in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0x2aa21e1 (0x7f29daa3e1e1 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #8: <unknown function> + 0xf3c44e (0x7f2a0a3bb44e in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #9: <unknown function> + 0xf3f811 (0x7f2a0a3be811 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #10: at::empty(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x302 (0x7f2a0a503532 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #11: at::TensorIterator::allocate_outputs() + 0x420 (0x7f2a0a0f4300 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #12: at::TensorIterator::build(at::TensorIteratorConfig&) + 0x1fe (0x7f2a0a0f608e in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #13: at::TensorIterator::TensorIterator(at::TensorIteratorConfig&) + 0xdd (0x7f2a0a0f66ad in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #14: at::TensorIterator::binary_op(at::Tensor&, at::Tensor const&, at::Tensor const&, bool) + 0x15d (0x7f2a0a0f686d in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #15: at::native::add(at::Tensor const&, at::Tensor const&, c10::Scalar) + 0x54 (0x7f2a09dd6a34 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x2a6286f (0x7f29da9fe86f in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #17: <unknown function> + 0x817ed8 (0x7f2a09c96ed8 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #18: at::add(at::Tensor const&, at::Tensor const&, c10::Scalar) + 0x164 (0x7f2a0a4c9514 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0x2525160 (0x7f2a0b9a4160 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #20: <unknown function> + 0x817ed8 (0x7f2a09c96ed8 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #21: at::Tensor::add(at::Tensor const&, c10::Scalar) const + 0x164 (0x7f2a0a67e6d4 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #22: <unknown function> + 0x29e58d8 (0x7f2a0be648d8 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #23: <unknown function> + 0x29e6804 (0x7f2a0be65804 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #24: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0xbbb (0x7f2a0be51f4b in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #25: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x4b4 (0x7f2a0be533f4 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #26: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x99 (0x7f2a0be4aec9 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #27: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x5a (0x7f2a10b8f05a in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so)
frame #28: <unknown function> + 0xbd6df (0x7f2a1cbc96df in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #29: <unknown function> + 0x76db (0x7f2a3f8186db in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #30: clone + 0x3f (0x7f2a3f541a3f in /lib/x86_64-linux-gnu/libc.so.6)

2020-10-23 18:09:05 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 10           |        cudaMalloc retries: 144       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   16183 MB |   21786 MB |   18182 GB |   18167 GB |
|       from large pool |   16181 MB |   21783 MB |   18143 GB |   18127 GB |
|       from small pool |       2 MB |     166 MB |      39 GB |      39 GB |
|---------------------------------------------------------------------------|
| Active memory         |   16183 MB |   21786 MB |   18182 GB |   18167 GB |
|       from large pool |   16181 MB |   21783 MB |   18143 GB |   18127 GB |
|       from small pool |       2 MB |     166 MB |      39 GB |      39 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21996 MB |   23436 MB |    1291 GB |    1269 GB |
|       from large pool |   21992 MB |   23416 MB |    1285 GB |    1264 GB |
|       from small pool |       4 MB |     170 MB |       5 GB |       5 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3870 MB |    7276 MB |   15626 GB |   15622 GB |
|       from large pool |    3868 MB |    7268 MB |   15582 GB |   15578 GB |
|       from small pool |       1 MB |      48 MB |      43 GB |      43 GB |
|---------------------------------------------------------------------------|
| Allocations           |    1605    |    1630    |    2541 K  |    2539 K  |
|       from large pool |     859    |     874    |    1645 K  |    1644 K  |
|       from small pool |     746    |     975    |     895 K  |     894 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1605    |    1630    |    2541 K  |    2539 K  |
|       from large pool |     859    |     874    |    1645 K  |    1644 K  |
|       from small pool |     746    |     975    |     895 K  |     894 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      49    |     214    |    4154    |    4105    |
|       from large pool |      47    |     189    |    1225    |    1178    |
|       from small pool |       2    |      85    |    2929    |    2927    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     114    |     182    |    1311 K  |    1311 K  |
|       from large pool |      62    |     132    |     924 K  |     924 K  |
|       from small pool |      52    |     108    |     387 K  |     387 K  |
|===========================================================================|

2020-10-23 18:09:05 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2020-10-23 18:09:16 | INFO | train_inner | epoch 001:    225 / 4418 loss=12.984, nll_loss=10.519, ppl=1467.33, wps=6467.2, ups=0.87, wpb=7470.6, bsz=197.4, num_updates=200, lr=8e-07, gnorm=91.436, loss_scale=0.0039, train_wall=11, wall=262
2020-10-23 18:09:26 | INFO | train_inner | epoch 001:    235 / 4418 loss=13.545, nll_loss=11.02, ppl=2076.55, wps=7026.4, ups=0.98, wpb=7194.2, bsz=249.3, num_updates=210, lr=8.4e-07, gnorm=117.469, loss_scale=0.0039, train_wall=10, wall=273

Usage example: change_mofed_version.sh 4.5-1.0.1
[10/23/2020, 06:16:58 PM] WARNING (nni) Requesting parameter without NNI framework, returning empty dict
2020-10-23 18:17:09 | INFO | fairseq_cli.train | Namespace(activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, allow_tf32=0, arch='mbart_large', attention_dropout=0.1, benchmark=0, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/research/d3/zmwu/model/mbart_company_version/post_process/en-zh_half/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_langtok=True, decoder_layerdrop=0, decoder_layers=12, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.0, empty_cache_freq=4, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_langtok='src', encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model='/research/d3/zmwu/model/mbart_company_version/mbart.cc25/model.pt', fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, inter=1, intra=2, keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=2, keep_last_epochs=-1, label_smoothing=0.1, lang_dict='/research/d3/zmwu/model/mbart_company_version/lang_list', lang_pairs='en_XX-zh_CN,zh_CN-en_XX', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=True, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format='simple', log_interval=10, lr=[1e-05], lr_scheduler='inverse_sqrt', max_epoch=1, max_sentences=1, max_sentences_valid=1, max_source_positions=1024, max_target_positions=1024, max_tokens=2048, max_tokens_valid=2048, max_update=20000, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='temperature', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='/research/d3/zmwu/model/mbart_company_version/ckpt/STANDALONE', save_interval=1, save_interval_updates=20000, scoring='bleu', seed=222, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch_nni', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[4], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir='/research/d3/zmwu/model/mbart_company_version/mbart/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=-1, warmup_updates=2500, weight_decay=0.0, zero_sharding='none')
2020-10-23 18:17:09 | INFO | fairseq.data.multilingual.multilingual_data_manager | loaded language list from /research/d3/zmwu/model/mbart_company_version/lang_list as they are ordered in file
2020-10-23 18:17:10 | INFO | fairseq.data.multilingual.multilingual_data_manager | [en_XX] dictionary: 250026 types
2020-10-23 18:17:10 | INFO | fairseq.data.multilingual.multilingual_data_manager | [zh_CN] dictionary: 250026 types
2020-10-23 18:17:10 | INFO | mbart.translation_multi_simple_epoch_nni | loading data for valid epoch=1/None
2020-10-23 18:17:10 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2020-10-23 18:17:10 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:en_XX-zh_CN': 1, 'main:zh_CN-en_XX': 1}
2020-10-23 18:17:10 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:en_XX-zh_CN src_langtok: 250004; tgt_langtok: 250025
2020-10-23 18:17:11 | INFO | fairseq.data.data_utils | loaded 1000000 examples from: /research/d3/zmwu/model/mbart_company_version/post_process/en-zh_half/valid.en_XX-zh_CN.en_XX
2020-10-23 18:17:11 | INFO | fairseq.data.data_utils | loaded 1000000 examples from: /research/d3/zmwu/model/mbart_company_version/post_process/en-zh_half/valid.en_XX-zh_CN.zh_CN
2020-10-23 18:17:11 | INFO | fairseq.data.multilingual.multilingual_data_manager | /research/d3/zmwu/model/mbart_company_version/post_process/en-zh_half/ valid en_XX-zh_CN 1000000 examples
2020-10-23 18:17:11 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:zh_CN-en_XX src_langtok: 250025; tgt_langtok: 250004
2020-10-23 18:17:11 | INFO | fairseq.data.data_utils | loaded 1000000 examples from: /research/d3/zmwu/model/mbart_company_version/post_process/en-zh_half/valid.en_XX-zh_CN.zh_CN
2020-10-23 18:17:11 | INFO | fairseq.data.data_utils | loaded 1000000 examples from: /research/d3/zmwu/model/mbart_company_version/post_process/en-zh_half/valid.en_XX-zh_CN.en_XX
2020-10-23 18:17:11 | INFO | fairseq.data.multilingual.multilingual_data_manager | /research/d3/zmwu/model/mbart_company_version/post_process/en-zh_half/ valid zh_CN-en_XX 1000000 examples
2020-10-23 18:17:34 | INFO | fairseq_cli.train | BARTModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(250026, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
    (layernorm_embedding): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(250026, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
    (layernorm_embedding): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=1024, out_features=250026, bias=False)
  )
  (classification_heads): ModuleDict()
)
2020-10-23 18:17:34 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch_nni (TranslationMultiSimpleEpochNNITask)
2020-10-23 18:17:34 | INFO | fairseq_cli.train | model: mbart_large (BARTModel)
2020-10-23 18:17:34 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-23 18:17:34 | INFO | fairseq_cli.train | num. model params: 610850816 (num. trained: 610850816)
2020-10-23 18:17:46 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-23 18:17:46 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-23 18:17:46 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-23 18:17:46 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2020-10-23 18:17:46 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-23 18:17:46 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-23 18:17:46 | INFO | fairseq_cli.train | max tokens per GPU = 2048 and max sentences per GPU = 1
2020-10-23 18:17:46 | INFO | fairseq.checkpoint_utils | loading pretrained model from /research/d3/zmwu/model/mbart_company_version/mbart.cc25/model.pt: optimizer, lr scheduler, meters, dataloader will be reset
2020-10-23 18:17:58 | INFO | fairseq.trainer | loaded checkpoint /research/d3/zmwu/model/mbart_company_version/mbart.cc25/model.pt (epoch 142 @ 0 updates)
2020-10-23 18:17:58 | INFO | fairseq.optim.adam | using FusedAdam
2020-10-23 18:17:58 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-23 18:17:58 | INFO | mbart.translation_multi_simple_epoch_nni | loading data for train epoch=1/None
2020-10-23 18:17:58 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2020-10-23 18:17:58 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:en_XX-zh_CN': 1, 'main:zh_CN-en_XX': 1}
2020-10-23 18:17:58 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:en_XX-zh_CN src_langtok: 250004; tgt_langtok: 250025
2020-10-23 18:18:00 | INFO | fairseq.data.data_utils | loaded 5900000 examples from: /research/d3/zmwu/model/mbart_company_version/post_process/en-zh_half/train.en_XX-zh_CN.en_XX
2020-10-23 18:18:01 | INFO | fairseq.data.data_utils | loaded 5900000 examples from: /research/d3/zmwu/model/mbart_company_version/post_process/en-zh_half/train.en_XX-zh_CN.zh_CN
2020-10-23 18:18:01 | INFO | fairseq.data.multilingual.multilingual_data_manager | /research/d3/zmwu/model/mbart_company_version/post_process/en-zh_half/ train en_XX-zh_CN 5900000 examples
2020-10-23 18:18:01 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:zh_CN-en_XX src_langtok: 250025; tgt_langtok: 250004
2020-10-23 18:18:02 | INFO | fairseq.data.data_utils | loaded 5900000 examples from: /research/d3/zmwu/model/mbart_company_version/post_process/en-zh_half/train.en_XX-zh_CN.zh_CN
2020-10-23 18:18:03 | INFO | fairseq.data.data_utils | loaded 5900000 examples from: /research/d3/zmwu/model/mbart_company_version/post_process/en-zh_half/train.en_XX-zh_CN.en_XX
2020-10-23 18:18:03 | INFO | fairseq.data.multilingual.multilingual_data_manager | /research/d3/zmwu/model/mbart_company_version/post_process/en-zh_half/ train zh_CN-en_XX 5900000 examples
2020-10-23 18:18:03 | INFO | fairseq.data.multilingual.multilingual_data_manager | data sizes multiplied by num_shards used in sampling ratios: [('main:en_XX-zh_CN', 5900000), ('main:zh_CN-en_XX', 5900000)]
2020-10-23 18:18:03 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: temperature
2020-10-23 18:18:03 | INFO | fairseq.data.multilingual.multilingual_data_manager | | Upsample ratios: [('main:en_XX-zh_CN', 0.6299605249474366), ('main:zh_CN-en_XX', 0.6299605249474366)]
2020-10-23 18:18:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000; virtual dataset size 11800000
2020-10-23 18:18:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-23 18:18:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:en_XX-zh_CN': 5900000, 'main:zh_CN-en_XX': 5900000}; raw total size: 11800000
2020-10-23 18:18:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:en_XX-zh_CN': 5900000, 'main:zh_CN-en_XX': 5900000}; resampled total size: 11800000
2020-10-23 18:18:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:en_XX-zh_CN': 0.6299605249474366, 'main:zh_CN-en_XX': 0.6299605249474366}
2020-10-23 18:18:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | sizes() calling time: 0:00:03.230806
2020-10-23 18:18:12 | WARNING | fairseq.tasks.fairseq_task | 10 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[639120, 199661, 195087, 831274, 290465, 233622, 29316, 547539, 697929, 942609]
2020-10-23 18:18:16 | INFO | fairseq.trainer | begin training epoch 1
2020-10-23 18:18:18 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-10-23 18:18:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0
2020-10-23 18:18:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0
2020-10-23 18:18:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8.0
2020-10-23 18:18:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4.0
2020-10-23 18:18:21 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2.0
2020-10-23 18:18:21 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1.0
2020-10-23 18:18:24 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.5
2020-10-23 18:18:25 | INFO | train_inner | epoch 001:     18 / 249998 loss=25.78, nll_loss=19.423, ppl=703028, wps=230.6, ups=1.8, wpb=124.2, bsz=4, num_updates=10, lr=4e-08, gnorm=564.076, loss_scale=0.5, train_wall=5, wall=38
2020-10-23 18:18:29 | INFO | train_inner | epoch 001:     28 / 249998 loss=26.488, nll_loss=20.261, ppl=1.25675e+06, wps=333.7, ups=2.55, wpb=130.8, bsz=4, num_updates=20, lr=8e-08, gnorm=569.954, loss_scale=0.5, train_wall=4, wall=42
2020-10-23 18:18:33 | INFO | train_inner | epoch 001:     38 / 249998 loss=29.431, nll_loss=23.669, ppl=1.33373e+07, wps=332.5, ups=2.54, wpb=130.9, bsz=4, num_updates=30, lr=1.2e-07, gnorm=959.699, loss_scale=0.5, train_wall=4, wall=46
2020-10-23 18:18:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.25
2020-10-23 18:18:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.125
2020-10-23 18:18:37 | INFO | train_inner | epoch 001:     50 / 249998 loss=26.763, nll_loss=21.301, ppl=2.58417e+06, wps=271.5, ups=2.14, wpb=127.1, bsz=4, num_updates=40, lr=1.6e-07, gnorm=798.689, loss_scale=0.125, train_wall=5, wall=51
2020-10-23 18:18:41 | INFO | train_inner | epoch 001:     60 / 249998 loss=22.943, nll_loss=18.027, ppl=267135, wps=315.7, ups=2.55, wpb=123.9, bsz=4, num_updates=50, lr=2e-07, gnorm=628.336, loss_scale=0.125, train_wall=4, wall=55
2020-10-23 18:18:42 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.0625
2020-10-23 18:18:46 | INFO | train_inner | epoch 001:     71 / 249998 loss=26.974, nll_loss=20.783, ppl=1.80377e+06, wps=346.7, ups=2.33, wpb=148.5, bsz=4, num_updates=60, lr=2.4e-07, gnorm=890.026, loss_scale=0.0625, train_wall=4, wall=59
2020-10-23 18:18:50 | INFO | train_inner | epoch 001:     81 / 249998 loss=23.997, nll_loss=19.029, ppl=535052, wps=295.6, ups=2.55, wpb=116, bsz=4, num_updates=70, lr=2.8e-07, gnorm=630.627, loss_scale=0.0625, train_wall=4, wall=63
2020-10-23 18:18:53 | INFO | train_inner | epoch 001:     91 / 249998 loss=25, nll_loss=18.998, ppl=523671, wps=308.6, ups=2.55, wpb=121, bsz=4, num_updates=80, lr=3.2e-07, gnorm=436.886, loss_scale=0.0625, train_wall=4, wall=67
2020-10-23 18:18:57 | INFO | train_inner | epoch 001:    101 / 249998 loss=24.363, nll_loss=19.489, ppl=735612, wps=277.6, ups=2.56, wpb=108.6, bsz=4, num_updates=90, lr=3.6e-07, gnorm=1952.09, loss_scale=0.0625, train_wall=4, wall=71
2020-10-23 18:19:01 | INFO | train_inner | epoch 001:    111 / 249998 loss=24.076, nll_loss=18.586, ppl=393617, wps=363.1, ups=2.54, wpb=142.7, bsz=4, num_updates=100, lr=4e-07, gnorm=2982.97, loss_scale=0.0625, train_wall=4, wall=75
2020-10-23 18:19:05 | INFO | train_inner | epoch 001:    121 / 249998 loss=18.845, nll_loss=14.68, ppl=26243.5, wps=305.1, ups=2.55, wpb=119.7, bsz=4, num_updates=110, lr=4.4e-07, gnorm=466.736, loss_scale=0.0625, train_wall=4, wall=79
2020-10-23 18:19:09 | INFO | train_inner | epoch 001:    131 / 249998 loss=19.793, nll_loss=15.529, ppl=47274, wps=320.8, ups=2.56, wpb=125.2, bsz=4, num_updates=120, lr=4.8e-07, gnorm=393.887, loss_scale=0.0625, train_wall=4, wall=83
2020-10-23 18:19:13 | INFO | train_inner | epoch 001:    141 / 249998 loss=19.507, nll_loss=14.83, ppl=29129.8, wps=315, ups=2.55, wpb=123.3, bsz=4, num_updates=130, lr=5.2e-07, gnorm=416.433, loss_scale=0.0625, train_wall=4, wall=87
2020-10-23 18:19:17 | INFO | train_inner | epoch 001:    151 / 249998 loss=17.495, nll_loss=13.602, ppl=12433.5, wps=309.9, ups=2.56, wpb=121.1, bsz=4, num_updates=140, lr=5.6e-07, gnorm=345.778, loss_scale=0.0625, train_wall=4, wall=91
2020-10-23 18:19:21 | INFO | train_inner | epoch 001:    161 / 249998 loss=15.317, nll_loss=12.238, ppl=4832.09, wps=294.5, ups=2.56, wpb=115.1, bsz=4, num_updates=150, lr=6e-07, gnorm=1063.13, loss_scale=0.0625, train_wall=4, wall=94
2020-10-23 18:19:25 | INFO | train_inner | epoch 001:    171 / 249998 loss=17.795, nll_loss=14.425, ppl=21999.8, wps=435.1, ups=2.47, wpb=176.4, bsz=4, num_updates=160, lr=6.4e-07, gnorm=594.02, loss_scale=0.0625, train_wall=4, wall=98
2020-10-23 18:19:29 | INFO | train_inner | epoch 001:    181 / 249998 loss=18.005, nll_loss=15.025, ppl=33331.1, wps=410.6, ups=2.47, wpb=166.1, bsz=4, num_updates=170, lr=6.8e-07, gnorm=368.719, loss_scale=0.0625, train_wall=4, wall=103
2020-10-23 18:19:33 | INFO | train_inner | epoch 001:    191 / 249998 loss=14.693, nll_loss=12.199, ppl=4703.08, wps=375.4, ups=2.55, wpb=147.4, bsz=4, num_updates=180, lr=7.2e-07, gnorm=301.159, loss_scale=0.0625, train_wall=4, wall=106
2020-10-23 18:19:37 | INFO | train_inner | epoch 001:    201 / 249998 loss=14.257, nll_loss=11.734, ppl=3405.54, wps=325.4, ups=2.56, wpb=127.3, bsz=4, num_updates=190, lr=7.6e-07, gnorm=151.323, loss_scale=0.0625, train_wall=4, wall=110
2020-10-23 18:19:41 | INFO | train_inner | epoch 001:    211 / 249998 loss=14.403, nll_loss=11.736, ppl=3411.75, wps=336, ups=2.52, wpb=133.3, bsz=4, num_updates=200, lr=8e-07, gnorm=231.041, loss_scale=0.0625, train_wall=4, wall=114
2020-10-23 18:19:45 | INFO | train_inner | epoch 001:    221 / 249998 loss=14.591, nll_loss=12.135, ppl=4497.11, wps=294.1, ups=2.55, wpb=115.1, bsz=4, num_updates=210, lr=8.4e-07, gnorm=150.084, loss_scale=0.0625, train_wall=4, wall=118
2020-10-23 18:19:49 | INFO | train_inner | epoch 001:    231 / 249998 loss=14.395, nll_loss=11.963, ppl=3992.9, wps=335.7, ups=2.56, wpb=131.2, bsz=4, num_updates=220, lr=8.8e-07, gnorm=122.828, loss_scale=0.0625, train_wall=4, wall=122
2020-10-23 18:19:53 | INFO | train_inner | epoch 001:    241 / 249998 loss=15.098, nll_loss=12.408, ppl=5433.74, wps=288.7, ups=2.55, wpb=113.2, bsz=4, num_updates=230, lr=9.2e-07, gnorm=658.437, loss_scale=0.0625, train_wall=4, wall=126
2020-10-23 18:19:56 | INFO | train_inner | epoch 001:    251 / 249998 loss=14.022, nll_loss=11.402, ppl=2706.29, wps=265.9, ups=2.56, wpb=104, bsz=4, num_updates=240, lr=9.6e-07, gnorm=128.156, loss_scale=0.0625, train_wall=4, wall=130
2020-10-23 18:20:00 | INFO | train_inner | epoch 001:    261 / 249998 loss=13.147, nll_loss=10.871, ppl=1872.33, wps=344.6, ups=2.54, wpb=135.8, bsz=4, num_updates=250, lr=1e-06, gnorm=1189.83, loss_scale=0.0625, train_wall=4, wall=134
2020-10-23 18:20:04 | INFO | train_inner | epoch 001:    271 / 249998 loss=12.867, nll_loss=10.655, ppl=1611.9, wps=352.7, ups=2.56, wpb=137.8, bsz=4, num_updates=260, lr=1.04e-06, gnorm=87.191, loss_scale=0.0625, train_wall=4, wall=138
2020-10-23 18:20:08 | INFO | train_inner | epoch 001:    281 / 249998 loss=13.228, nll_loss=10.969, ppl=2004.92, wps=354.8, ups=2.53, wpb=140.1, bsz=4, num_updates=270, lr=1.08e-06, gnorm=125.36, loss_scale=0.0625, train_wall=4, wall=142
2020-10-23 18:20:12 | INFO | train_inner | epoch 001:    291 / 249998 loss=13.785, nll_loss=11.267, ppl=2464.04, wps=292.6, ups=2.56, wpb=114.2, bsz=4, num_updates=280, lr=1.12e-06, gnorm=148.73, loss_scale=0.0625, train_wall=4, wall=146
2020-10-23 18:20:16 | INFO | train_inner | epoch 001:    301 / 249998 loss=12.989, nll_loss=10.573, ppl=1523.67, wps=349.8, ups=2.53, wpb=138.5, bsz=4, num_updates=290, lr=1.16e-06, gnorm=97.222, loss_scale=0.0625, train_wall=4, wall=150
2020-10-23 18:20:20 | INFO | train_inner | epoch 001:    311 / 249998 loss=12.663, nll_loss=10.324, ppl=1281.53, wps=329, ups=2.56, wpb=128.6, bsz=4, num_updates=300, lr=1.2e-06, gnorm=119.346, loss_scale=0.0625, train_wall=4, wall=154
2020-10-23 18:20:24 | INFO | train_inner | epoch 001:    321 / 249998 loss=13.19, nll_loss=11.048, ppl=2116.74, wps=314.4, ups=2.53, wpb=124.2, bsz=4, num_updates=310, lr=1.24e-06, gnorm=421.957, loss_scale=0.0625, train_wall=4, wall=158
2020-10-23 18:20:28 | INFO | train_inner | epoch 001:    331 / 249998 loss=13.369, nll_loss=10.995, ppl=2040.56, wps=329, ups=2.56, wpb=128.4, bsz=4, num_updates=320, lr=1.28e-06, gnorm=160.306, loss_scale=0.0625, train_wall=4, wall=161
2020-10-23 18:20:32 | INFO | train_inner | epoch 001:    341 / 249998 loss=13.403, nll_loss=10.896, ppl=1905.83, wps=254.2, ups=2.54, wpb=100, bsz=4, num_updates=330, lr=1.32e-06, gnorm=87.971, loss_scale=0.0625, train_wall=4, wall=165
2020-10-23 18:20:36 | INFO | train_inner | epoch 001:    351 / 249998 loss=12.725, nll_loss=10.371, ppl=1324.42, wps=306.4, ups=2.56, wpb=119.6, bsz=4, num_updates=340, lr=1.36e-06, gnorm=114.57, loss_scale=0.0625, train_wall=4, wall=169
2020-10-23 18:20:40 | INFO | train_inner | epoch 001:    361 / 249998 loss=13.284, nll_loss=10.975, ppl=2012.88, wps=273.7, ups=2.56, wpb=107.1, bsz=4, num_updates=350, lr=1.4e-06, gnorm=80.537, loss_scale=0.0625, train_wall=4, wall=173
2020-10-23 18:20:44 | INFO | train_inner | epoch 001:    371 / 249998 loss=12.961, nll_loss=10.639, ppl=1594.73, wps=262.6, ups=2.41, wpb=108.9, bsz=4, num_updates=360, lr=1.44e-06, gnorm=81.605, loss_scale=0.0625, train_wall=4, wall=177
2020-10-23 18:20:48 | INFO | train_inner | epoch 001:    381 / 249998 loss=11.788, nll_loss=9.471, ppl=709.6, wps=380.1, ups=2.49, wpb=152.5, bsz=4, num_updates=370, lr=1.48e-06, gnorm=362.847, loss_scale=0.0625, train_wall=4, wall=181
2020-10-23 18:20:52 | INFO | train_inner | epoch 001:    391 / 249998 loss=12.806, nll_loss=10.555, ppl=1504.43, wps=294.5, ups=2.56, wpb=114.9, bsz=4, num_updates=380, lr=1.52e-06, gnorm=542.85, loss_scale=0.0625, train_wall=4, wall=185
2020-10-23 18:20:56 | INFO | train_inner | epoch 001:    401 / 249998 loss=12.408, nll_loss=10.333, ppl=1289.6, wps=373.1, ups=2.5, wpb=149.1, bsz=4, num_updates=390, lr=1.56e-06, gnorm=71.073, loss_scale=0.0625, train_wall=4, wall=189
2020-10-23 18:21:00 | INFO | train_inner | epoch 001:    411 / 249998 loss=12.172, nll_loss=9.826, ppl=907.34, wps=322.3, ups=2.56, wpb=126.1, bsz=4, num_updates=400, lr=1.6e-06, gnorm=157.508, loss_scale=0.0625, train_wall=4, wall=193
2020-10-23 18:21:04 | INFO | train_inner | epoch 001:    421 / 249998 loss=11.787, nll_loss=9.646, ppl=801.36, wps=367.2, ups=2.54, wpb=144.4, bsz=4, num_updates=410, lr=1.64e-06, gnorm=77.411, loss_scale=0.0625, train_wall=4, wall=197
2020-10-23 18:21:07 | INFO | train_inner | epoch 001:    431 / 249998 loss=11.762, nll_loss=9.539, ppl=743.86, wps=296.5, ups=2.54, wpb=116.9, bsz=4, num_updates=420, lr=1.68e-06, gnorm=131.546, loss_scale=0.0625, train_wall=4, wall=201
2020-10-23 18:21:11 | INFO | train_inner | epoch 001:    441 / 249998 loss=11.715, nll_loss=9.56, ppl=754.76, wps=355.3, ups=2.54, wpb=139.8, bsz=4, num_updates=430, lr=1.72e-06, gnorm=73.667, loss_scale=0.0625, train_wall=4, wall=205
2020-10-23 18:21:15 | INFO | train_inner | epoch 001:    451 / 249998 loss=12.05, nll_loss=9.837, ppl=914.73, wps=314, ups=2.56, wpb=122.5, bsz=4, num_updates=440, lr=1.76e-06, gnorm=90.719, loss_scale=0.0625, train_wall=4, wall=209
2020-10-23 18:21:19 | INFO | train_inner | epoch 001:    461 / 249998 loss=12.152, nll_loss=9.903, ppl=957.7, wps=305.1, ups=2.54, wpb=120, bsz=4, num_updates=450, lr=1.8e-06, gnorm=67.963, loss_scale=0.0625, train_wall=4, wall=213
2020-10-23 18:21:23 | INFO | train_inner | epoch 001:    471 / 249998 loss=12.022, nll_loss=9.821, ppl=904.62, wps=299.2, ups=2.56, wpb=116.9, bsz=4, num_updates=460, lr=1.84e-06, gnorm=357.745, loss_scale=0.0625, train_wall=4, wall=217
2020-10-23 18:21:27 | INFO | train_inner | epoch 001:    481 / 249998 loss=12.296, nll_loss=10.158, ppl=1142.73, wps=331.1, ups=2.52, wpb=131.3, bsz=4, num_updates=470, lr=1.88e-06, gnorm=118.673, loss_scale=0.0625, train_wall=4, wall=221
